{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DFIR-Metric\n",
        "\n",
        "##Module III - NIST Computer Forensics Tool Testing Program (CFTT) Forensic String Search\n",
        "\n",
        "###Authors: Bilel Cherif, Tamas Bisztray, Richard A. Dubniczky, Aaesha Aldahmani, Saeed Alshehhi and Norbert Tihanyi\n"
      ],
      "metadata": {
        "id": "HJtq0e-sXlGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 0: Install packages and download the NIST String-Search dataset\n",
        "from tqdm import tqdm\n",
        "import subprocess, sys, os, requests\n",
        "\n",
        "\n",
        "#Install pip packages\n",
        "requirements_txt = \"\"\"\n",
        "spire-doc\n",
        "pytsk3\n",
        "pandas\n",
        "requests\n",
        "\"\"\"\n",
        "pip_packages = requirements_txt.strip().split(\"\\n\")\n",
        "\n",
        "print(\"[*] Installing Python packages...\")\n",
        "for package in tqdm(pip_packages, desc=\"[*] Pip Packages\"):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\"],\n",
        "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "print(\"[*] Python packages installation: DONE\")\n",
        "\n",
        "#Download NIST dataset from the official repo\n",
        "url = \"https://cfreds-archive.nist.gov/StringSearching/string-search-federated-testing-data-set-version-1-1-revised-september-27-2019.zip\"\n",
        "local_zip = \"string_search_dataset.zip\"\n",
        "\n",
        "# Skip download if already present\n",
        "if not os.path.exists(local_zip):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total = int(response.headers.get(\"content-length\", 0))\n",
        "    with open(local_zip, \"wb\") as f, tqdm(\n",
        "            desc=\"[*] Downloading ZIP\",\n",
        "            total=total,\n",
        "            unit=\"iB\",\n",
        "            unit_scale=True,\n",
        "    ) as bar:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                bar.update(len(chunk))\n",
        "    print(\"[+] Download : DONE\")\n",
        "else:\n",
        "    print(f\"[*] {local_zip} already exists: skipping download\")\n",
        "\n",
        "# Extract the dataset\n",
        "extract_dir = \"string_search_dataset\"\n",
        "if not os.path.isdir(extract_dir):\n",
        "    print(f\"[*] Extracting...\")\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(local_zip, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "    print(\"[+] Extraction: DONE\")\n",
        "else:\n",
        "    print(f\"[*] {extract_dir}/ already exists: skipping extraction\")\n",
        "\n",
        "print(\"[*] All steps: DONE\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp9MyHROSG7C",
        "outputId": "b5305d46-71b2-4023-b41d-3b33e4d0e59b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Installing Python packages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*] Pip Packages: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Python packages installation: DONE\n",
            "[*] string_search_dataset.zip already exists: skipping download\n",
            "[*] string_search_dataset/ already exists: skipping extraction\n",
            "[*] All steps: DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "E5Lveck2Wll9"
      },
      "outputs": [],
      "source": [
        "#STEP 1: Generate the DFIR-Metric-NSS.json form the NIST images (ss-win-07-25-18.dd and ss-unix-07-25-18.dd)\n",
        "import pytsk3\n",
        "import sys\n",
        "from spire.doc import *\n",
        "from spire.doc.common import *\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "class StringSearchBench():\n",
        "    def __init__(self):\n",
        "        self.NIST_dataset_path = \"/content/string_search_dataset/string-search-federated-testing-data-set-version-1-1-revised-september-27-2019\"\n",
        "        self.win_partions_dict = {\"the first windows data partion\": 34, \"the second windows data partion\": 1953124, \"the third windows data partion\": 2929686}\n",
        "        self.unix_partions_dict = {\"the linux filesystem\": 978944, \"the first HFS+\": 2048, \"the second HFS+\": 1955840}\n",
        "        self.win_partions = [\"the first windows data partion\", \"the second windows data partion\", \"the third windows data partion\"]\n",
        "        self.unix_partitions = [\"the linux filesystem\", \"the first HFS+\", \"the second HFS+\"]\n",
        "        self.emails = [\"iron.man@marvel.com\", \"potus@capitol.gov\", \"berlin@deutchland.net\", \"kgb@moscow.red.square.ru\"]\n",
        "        self.phones = [\"\\(901\\)555-1111\", \" 301.555-9009\", \"800-555-1122\", \"202.555.3270\"]\n",
        "        self.extensions = {\"text\": \".txt\", \"web\": \".html\", \"word\": \".doc and .docx\", \"all\": \".txt, .html, .doc and .docx\"}\n",
        "        self.words = [\"Wolf\", \"thunderbird\", \"DireWolf\" ,\"garçon\",  \"الكسكس\", \"فلافل\", b\"\\x70\\x61\\x6e\\x64\\x61\", \"shotgun\", \"flintlock\", \"rifle\", \"revolver\", \"longbow\", \"crossbow\", \"fox\", \"tiger\"]\n",
        "        self.words_condition = [[\"shotgun\", \"flintlock\", \"rifle\"], [\"peroxide\", \"nitroglycerin\"], [\"revolver\", \"longbow\", \"crossbow\"],[\"panda\", \"fox\"], [\"fox\", \"tiger\"], [\"panda\", \"tiger\", \"fox\"]]\n",
        "        self.pattern = [\"email\", \"Phone Number\", \"Social Security Number\"]\n",
        "        self.operators=[\"or\", \"and\"]\n",
        "        self.case_sensitivity = [True, False]\n",
        "        self.whole_word = [True, False]\n",
        "        self.file_state = [\"deleted and non deleted\", \"deleted\", \"non deleted\"]\n",
        "        self.win_img = self.NIST_dataset_path + \"/copy-to-test-computer/ss-win-07-25-18.dd\"\n",
        "        self.unix_img = self.NIST_dataset_path + \"/copy-to-test-computer/ss-unix-07-25-18.dd\"\n",
        "\n",
        "        self.unix_mmls = \"\"\"\n",
        "The mmls commant output for ss-unix-07-25-18.dd is:\n",
        "mmls ss-unix-07-25-18.dd\n",
        "GUID Partition Table (EFI)\n",
        "Offset Sector: 0\n",
        "Units are in 512-byte sectors\n",
        "Slot      Start        End          Length       Description\n",
        "000:  Meta      0000000000   0000000000   0000000001   Safety Table\n",
        "001:  -------   0000000000   0000002047   0000002048   Unallocated\n",
        "002:  Meta      0000000001   0000000001   0000000001   GPT Header\n",
        "003:  Meta      0000000002   0000000033   0000000032   Partition Table\n",
        "004:  000       0000002048   0000978610   0000976563   OS X Hierarchical File System Plus (HFS+) partition\n",
        "005:  -------   0000978611   0000978943   0000000333   Unallocated\n",
        "006:  001       0000978944   0001955506   0000976563   Linux filesystem data\n",
        "007:  -------   0001955507   0001955839   0000000333   Unallocated\n",
        "008:  002       0001955840   0002932402   0000976563   OS X Hierarchical File System Plus (HFS+) partition\n",
        "009:  -------   0002932403   0002932735   0000000333   Unallocated\n",
        "010:  003       0002932736   0004097671   0001164936\n",
        "011:  -------   0004097672   0004097711   0000000040   Unallocated\n",
        "        \"\"\"\n",
        "\n",
        "        self.win_mmls = \"\"\"\n",
        "The mmls commant output for ss-win-07-25-18.dd is:\n",
        "mmls ss-win-07-25-18.dd\n",
        "GUID Partition Table (EFI)\n",
        "Offset Sector: 0\n",
        "Units are in 512-byte sectors\n",
        "\n",
        "    Slot      Start        End          Length       Description\n",
        "000:  Meta      0000000000   0000000000   0000000001   Safety Table\n",
        "001:  -------   0000000000   0000000033   0000000034   Unallocated\n",
        "002:  Meta      0000000001   0000000001   0000000001   GPT Header\n",
        "003:  Meta      0000000002   0000000033   0000000032   Partition Table\n",
        "004:  000       0000000034   0000976561   0000976528   Basic data partition\n",
        "005:  001       0000976562   0001953123   0000976562   Basic data partition\n",
        "006:  002       0001953124   0002929685   0000976562   Basic data partition\n",
        "007:  003       0002929686   0003906247   0000976562   Basic data partition\n",
        "008:  -------   0003906248   0004097711   0000191464   Unallocated\n",
        "        \"\"\"\n",
        "\n",
        "        self.email_prompt = \"\"\"\n",
        "The following is the output of mmls on a disk image. Write a script to find all {} files that contain the email {} in {}.\n",
        "The output should be a list sorted by inode in the format: <xml>[\"inode:filename\", \"inode:filename\", ...]</xml>.\n",
        "Deleted files should have filenames starting with DELETED, and non-deleted files should start with LIVE.\n",
        "For example: <xml>[\"122150:DELETED-test-email.txt\", \"122152:LIVE-test-email.txt\"]</xml>.\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "        self.phone_prompt = \"\"\"\n",
        "The following is the output of mmls on a disk image. Write a script to find all {} files that contain the phone number {} in {}.\n",
        "The output should be a list sorted by inode in the format: <xml>[\"inode:filename\", \"inode:filename\", ...]</xml>.\n",
        "Deleted files should have filenames starting with DELETED, and non-deleted files should start with LIVE.\n",
        "For example: <xml>\"130735:DELETED-phone-examplefile.txt\", \"122152:LIVE-phone-file2.txt\"</xml>.\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "        self.text_prompt = \"\"\"\n",
        "The following is the output of mmls on a disk image. Write a script to find all the {} files that contain the word/words {} in {}.\n",
        "The output should be a list sorted by inode in the format: <xml>[\"inode:filename\", \"inode:filename\", ...]</xml>.\n",
        "Deleted files should have filenames starting with DELETED, and non-deleted files should start with LIVE.\n",
        "For example: <xml>\"122150:DELETED-example-ascii.txt\", \"122152:LIVE-example-fat-ascii.txt\"</xml>.\n",
        "{}\n",
        "\n",
        "        \"\"\"\n",
        "        self.extension_count_prompt = \"\"\"\n",
        "The following is the output of mmls on a disk image. Write a script to count all the {} files with the extension(s) {} in {}.\n",
        "The output format should comply with this format: <count>\n",
        "Count is the number of identified files\n",
        "The deleted files start with DELETED the non deleted files start with LIVE\n",
        "{}\n",
        "        \"\"\"\n",
        "        self.extension_count_prompt = \"\"\"\n",
        "The following is the output of mmls on a disk image. Write a script to count all {} files with the extension(s) {} in {}.\n",
        "The output should be a single value in the format: <xml>count</xml>, representing the number of identified files.\n",
        "Deleted files should have filenames starting with DELETED, and non-deleted files should start with LIVE. For example: <xml>211</xml>\n",
        "        {}\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def matches(self, text, keywords, match_whole_words=False, case_sensitive=False, logic_operator=\"none\"):\n",
        "        # Choose flags\n",
        "        flags = 0 if case_sensitive else re.IGNORECASE\n",
        "\n",
        "        # Prepare final match pattern\n",
        "        if isinstance(keywords, list):\n",
        "            if logic_operator == \"or\":\n",
        "                # Match any of the words (whole words if match_whole_words is True)\n",
        "                word_pattern = '\\\\b(?:' + '|'.join(re.escape(word) for word in keywords) + ')\\\\b' if match_whole_words else '|'.join(re.escape(word) for word in keywords)\n",
        "                pattern = word_pattern\n",
        "            elif logic_operator == \"and\":\n",
        "                # Match all the words (whole words if match_whole_words is True)\n",
        "                word_pattern = '\\\\b(?:' + '|'.join(re.escape(word) for word in keywords) + ')\\\\b' if match_whole_words else '|'.join(re.escape(word) for word in keywords)\n",
        "                pattern = '^.*' + '.*'.join(f'(?=.*\\\\b{re.escape(word)}\\\\b)' for word in keywords) + '.*$'\n",
        "            else:\n",
        "                # Default to \"none\", where any of the words are matched\n",
        "                word_pattern = '\\\\b(?:' + '|'.join(re.escape(word) for word in keywords) + ')\\\\b' if match_whole_words else '|'.join(re.escape(word) for word in keywords)\n",
        "                pattern = word_pattern\n",
        "        else:\n",
        "            # If keywords is a single string, use it as is\n",
        "            pattern = keywords\n",
        "\n",
        "        # Perform regex search\n",
        "        if re.search(pattern.encode(), text, flags=flags):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def detect_utf16(self, file_data):\n",
        "        ENCODING_UTF_16_BE = 'utf_16_be'\n",
        "        ENCODING_UTF_16_LE = 'utf_16_le'\n",
        "\n",
        "        # http://unicode.org/faq/utf_bom.html#BOM\n",
        "        BOM_UTF_16_BE = b'\\xfe\\xff'\n",
        "        BOM_UTF_16_LE = b'\\xff\\xfe'\n",
        "\n",
        "        BYTE_EOL = (13,10) # \\r\\n\n",
        "\n",
        "        UTF_16_NULL_PERCENT_POSITIVE = 0.7\n",
        "        UTF_16_NULL_PERCENT_NEGATIVE = 0.1\n",
        "        null_byte_odd,null_byte_even = 0,0\n",
        "        eol_odd,eol_even = 0,0\n",
        "        if file_data[:2] == BOM_UTF_16_BE:\n",
        "            return ENCODING_UTF_16_BE\n",
        "\n",
        "        elif file_data[:2] == BOM_UTF_16_LE:\n",
        "            return ENCODING_UTF_16_LE\n",
        "\n",
        "        else:\n",
        "            odd_byte = None\n",
        "            for file_byte in file_data:\n",
        "                # build pairs of bytes\n",
        "                if (odd_byte is None):\n",
        "                    odd_byte = file_byte\n",
        "                    continue\n",
        "\n",
        "                # look for odd/even null byte and check other byte for EOL\n",
        "                if (odd_byte == 0):\n",
        "                    null_byte_odd += 1\n",
        "                    if (file_byte in BYTE_EOL):\n",
        "                        eol_even += 1\n",
        "\n",
        "                elif (file_byte == 0):\n",
        "                    null_byte_even += 1\n",
        "                    if (odd_byte in BYTE_EOL):\n",
        "                        eol_odd += 1\n",
        "\n",
        "                odd_byte = None\n",
        "\n",
        "            # attempt detection based on line endings\n",
        "            if ((not eol_odd) and eol_even):\n",
        "                return ENCODING_UTF_16_BE\n",
        "\n",
        "            if (eol_odd and (not eol_even)):\n",
        "                return ENCODING_UTF_16_LE\n",
        "\n",
        "            # can't detect on line endings - evaluate ratio of null bytes in odd/even positions\n",
        "            # this will give an indication of how much ASCII (1-127) level text is present\n",
        "            data_size_half = (len(file_data) / 2)\n",
        "            threshold_positive = int(data_size_half * UTF_16_NULL_PERCENT_POSITIVE)\n",
        "            threshold_negative = int(data_size_half * UTF_16_NULL_PERCENT_NEGATIVE)\n",
        "\n",
        "            # must have enough file data to have value ([threshold_positive] must be non-zero)\n",
        "            if (threshold_positive):\n",
        "                if ((null_byte_odd > threshold_positive) and (null_byte_even < threshold_negative)):\n",
        "                    return ENCODING_UTF_16_LE\n",
        "\n",
        "                if ((null_byte_odd < threshold_negative) and (null_byte_even > threshold_positive)):\n",
        "                    return ENCODING_UTF_16_BE\n",
        "\n",
        "            # not UTF-16 - or insufficient data to determine with confidence\n",
        "            return False\n",
        "\n",
        "    def search_in_image(self, image_path, hex_or_ascii, start_sector=34, sector_size=512, _match_whole_words = True, _case_sensitive = False, _logic_operator = \"or\", extension = \"all\", mode=\"search\"):\n",
        "        if isinstance(hex_or_ascii, list):\n",
        "            hex_or_ascii = hex_or_ascii\n",
        "        elif isinstance(hex_or_ascii, bytes) :\n",
        "                hex_or_ascii = hex_or_ascii.decode()\n",
        "        hex_or_ascii_bytes = hex_or_ascii\n",
        "        result = []\n",
        "        #print(f\"[*] Searching for files containing '{hex_or_ascii}'...\")\n",
        "\n",
        "        # Open image file\n",
        "        img_info = pytsk3.Img_Info(image_path)\n",
        "        fs_info = pytsk3.FS_Info(img_info, offset=start_sector * sector_size)\n",
        "        def walk(directory, result):\n",
        "            for entry in directory:\n",
        "                if not hasattr(entry, \"info\") or not hasattr(entry.info, \"name\"):\n",
        "                    continue\n",
        "\n",
        "                name = entry.info.name.name.decode(\"utf-8\", errors=\"ignore\")\n",
        "                if name in [\".\", \"..\"]:\n",
        "                    continue\n",
        "\n",
        "                meta = entry.info.meta\n",
        "                if meta is None:\n",
        "                    continue\n",
        "\n",
        "                # Include both allocated and unallocated files\n",
        "                is_allocated = bool(meta.flags & pytsk3.TSK_FS_META_FLAG_ALLOC)\n",
        "                is_unallocated = bool(meta.flags & pytsk3.TSK_FS_META_FLAG_UNALLOC)\n",
        "                if meta.type == pytsk3.TSK_FS_META_TYPE_DIR:\n",
        "                    try:\n",
        "                        walk(entry.as_directory(), result)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                elif meta.size > 0 and (is_allocated or is_unallocated):\n",
        "                    if extension == \"all\":\n",
        "                        #print(f\"Doc file: {name}\")\n",
        "                        if name.endswith(\".doc\") or name.endswith(\".docx\"):\n",
        "                            filedata = entry.read_random(0, meta.size)\n",
        "                            with open(self.NIST_dataset_path + \"/copy-to-test-computer/temp/\" + name, \"wb\") as f:\n",
        "                                f.write(filedata)\n",
        "                            document = Document()\n",
        "\n",
        "                            # Load a Word file from disk\n",
        "                            document.LoadFromFile(\"./copy-to-test-computer/temp/\" + name)\n",
        "                            text = document.GetText()\n",
        "                            if self.matches(text.encode(), hex_or_ascii_bytes, match_whole_words = _match_whole_words, case_sensitive = _case_sensitive, logic_operator = _logic_operator):\n",
        "                                alloc_status = \"deleted\" if is_unallocated else \"allocated\"\n",
        "                                #print(f\"{meta.addr}:{name} [{alloc_status}]\")\n",
        "                                result.append(f\"{meta.addr}:{name}#{alloc_status}\")\n",
        "                        else:\n",
        "                            filedata = entry.read_random(0, meta.size)\n",
        "                            if name.endswith(\".txt\"):\n",
        "                                encoding = self.detect_utf16(filedata)\n",
        "\n",
        "                                if encoding == \"utf_16_be\" :\n",
        "                                    filedata = filedata.decode(\"utf-16be\")\n",
        "                                    filedata = filedata.encode(\"utf-8\")\n",
        "                                elif encoding == \"utf_16_le\":\n",
        "                                    filedata = filedata.decode(\"utf-16le\")\n",
        "                                    filedata = filedata.encode(\"utf-8\")\n",
        "\n",
        "                            if self.matches(filedata, hex_or_ascii_bytes, match_whole_words = _match_whole_words, case_sensitive = _case_sensitive, logic_operator = _logic_operator):\n",
        "                                alloc_status = \"deleted\" if is_unallocated else \"allocated\"\n",
        "                                #print(f\"{meta.addr}:{name} [{alloc_status}]\")\n",
        "                                result.append(f\"{meta.addr}:{name}#{alloc_status}\")\n",
        "                    elif extension == \"word\":\n",
        "                        if name.endswith(\".doc\") or name.endswith(\".docx\"):\n",
        "                            #print(f\"Doc file: {name}\")\n",
        "                            filedata = entry.read_random(0, meta.size)\n",
        "                            with open(\"./copy-to-test-computer/temp/\" + name, \"wb\") as f:\n",
        "                                f.write(filedata)\n",
        "                            document = Document()\n",
        "\n",
        "                            # Load a Word file from disk\n",
        "                            document.LoadFromFile(\"./copy-to-test-computer/temp/\" + name)\n",
        "                            text = document.GetText()\n",
        "                            if self.matches(text.encode(), hex_or_ascii_bytes, match_whole_words = _match_whole_words, case_sensitive = _case_sensitive, logic_operator = _logic_operator):\n",
        "                                alloc_status = \"deleted\" if is_unallocated else \"allocated\"\n",
        "                                #print(f\"{meta.addr}:{name} [{alloc_status}]\")\n",
        "                                result.append(f\"{meta.addr}:{name}#{alloc_status}\")\n",
        "                    elif extension == \"web\":\n",
        "                        if name.endswith(\".html\"):\n",
        "                            filedata = entry.read_random(0, meta.size)\n",
        "                            if self.matches(filedata, hex_or_ascii_bytes, match_whole_words = _match_whole_words, case_sensitive = _case_sensitive, logic_operator = _logic_operator):\n",
        "                                alloc_status = \"deleted\" if is_unallocated else \"allocated\"\n",
        "                                #print(f\"{meta.addr}:{name} [{alloc_status}]\")\n",
        "                                result.append(f\"{meta.addr}:{name}#{alloc_status}\")\n",
        "                    elif extension == \"text\":\n",
        "                        if name.endswith(\".txt\"):\n",
        "                            filedata = entry.read_random(0, meta.size)\n",
        "                            if self.matches(filedata, hex_or_ascii_bytes, match_whole_words = _match_whole_words, case_sensitive = _case_sensitive, logic_operator = _logic_operator):\n",
        "                                alloc_status = \"deleted\" if is_unallocated else \"allocated\"\n",
        "                                #print(f\"{meta.addr}:{name} [{alloc_status}]\")\n",
        "                                result.append(f\"{meta.addr}:{name}#{alloc_status}\")\n",
        "\n",
        "        def walk_count(directory, count):\n",
        "            count = count\n",
        "            for entry in directory:\n",
        "                if not hasattr(entry, \"info\") or not hasattr(entry.info, \"name\"):\n",
        "                    continue\n",
        "\n",
        "                name = entry.info.name.name.decode(\"utf-8\", errors=\"ignore\")\n",
        "                if name in [\".\", \"..\"]:\n",
        "                    continue\n",
        "\n",
        "                meta = entry.info.meta\n",
        "                if meta is None:\n",
        "                    continue\n",
        "\n",
        "                # Include both allocated and unallocated files\n",
        "                is_allocated = bool(meta.flags & pytsk3.TSK_FS_META_FLAG_ALLOC)\n",
        "                is_unallocated = bool(meta.flags & pytsk3.TSK_FS_META_FLAG_UNALLOC)\n",
        "\n",
        "                if meta.size > 0 and (is_allocated or is_unallocated):\n",
        "                    if extension == \"all\":\n",
        "                        count +=1\n",
        "                    elif extension == \"word\":\n",
        "                        if name.endswith(\".doc\") or name.endswith(\".docx\"):\n",
        "                            count +=1\n",
        "                    elif extension == \"web\":\n",
        "                        if name.endswith(\".html\"):\n",
        "                            count+=1\n",
        "                    elif extension == \"text\":\n",
        "                        if name.endswith(\".txt\"):\n",
        "                            count+=1\n",
        "                if meta.type == pytsk3.TSK_FS_META_TYPE_DIR:\n",
        "                    try:\n",
        "                        count = walk_count(entry.as_directory(), count)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "            return count\n",
        "\n",
        "        directory = fs_info.open_dir(\"/\")\n",
        "        if mode == \"search\":\n",
        "            walk(directory, result)\n",
        "            return result\n",
        "        elif mode == \"count\":\n",
        "            count = 0\n",
        "            count = walk_count(directory, count)\n",
        "            return f\"{count}\"\n",
        "\n",
        "    def gen_prompt(self, kind, mmls, partition,target, state, extension, operator=\"all\"):\n",
        "        if kind == \"email\":\n",
        "            prompt = self.email_prompt.format(state, target, partition, mmls)\n",
        "        elif kind == \"phone\":\n",
        "            prompt = self.phone_prompt.format(state, target, partition, mmls)\n",
        "        elif kind == \"string\":\n",
        "            prompt = self.text_prompt.format(state, target, partition, mmls)\n",
        "        elif kind == \"condition\":\n",
        "            if operator == \"or\":\n",
        "                temp_target = \" or \".join(target)\n",
        "            elif operator == \"and\":\n",
        "                temp_target = \" and \".join(target)\n",
        "            prompt = self.text_prompt.format(state, temp_target, partition, mmls)\n",
        "        elif kind == \"count\":\n",
        "            prompt = self.extension_count_prompt.format(state, extension, partition, mmls)\n",
        "        return prompt\n",
        "\n",
        "    def filter_state(self, state, baseline):\n",
        "        temp_base = []\n",
        "        if len(baseline) > 0:\n",
        "            if state == \"deleted\":\n",
        "                for base in baseline:\n",
        "                    _base = base.split(\"#\")\n",
        "                    if _base[-1] == \"deleted\":\n",
        "                        temp_base.append(_base[0])\n",
        "            elif state == \"non deleted\":\n",
        "                for base in baseline:\n",
        "                    _base = base.split(\"#\")\n",
        "                    if _base[-1] != \"deleted\":\n",
        "                        temp_base.append(_base[0])\n",
        "            else:\n",
        "                for base in baseline:\n",
        "                    _base = base.split(\"#\")\n",
        "                    temp_base.append(_base[0])\n",
        "\n",
        "        else:\n",
        "            temp_base = baseline\n",
        "\n",
        "        return temp_base\n",
        "\n",
        "    def generate_baseline(self, os, __mode, target = \"wolf\",operator = \"or\", extension = \"all\", case_sensitivity=False, whole_word=False, _start_sector = 34):\n",
        "        if os == \"win\":\n",
        "            return self.search_in_image(self.win_img, target, start_sector=_start_sector, sector_size=512, _match_whole_words = whole_word, _case_sensitive = case_sensitivity, _logic_operator = operator, extension = extension, mode = __mode)\n",
        "        elif os == \"unix\":\n",
        "            return self.search_in_image(self.unix_img, target, start_sector=_start_sector, sector_size=512, _match_whole_words = whole_word, _case_sensitive = case_sensitivity, _logic_operator = operator, extension = extension, mode= __mode)\n",
        "\n",
        "    def win_gen(self, kind, state, target, partition, case_sensitivity, whole_word, extension, operator, _mode):\n",
        "        elements = {}\n",
        "        if _mode == \"search\":\n",
        "            question = self.gen_prompt(kind, self.win_mmls, partition, target, state, extension, operator = operator)\n",
        "            baseline = self.generate_baseline(\"win\", _mode, target=target, _start_sector=self.win_partions_dict[partition], case_sensitivity=case_sensitivity, whole_word=whole_word, extension=extension, operator=operator)\n",
        "            baseline = self.filter_state(state, baseline)\n",
        "            elements[\"offset\"] = f\"{self.win_partions_dict[partition]}\"\n",
        "            elements[\"image\"] = \"ss-win-07-25-18.dd\"\n",
        "            elements[\"target\"] = f\"{target}\"\n",
        "            elements[\"extension\"] = f\"{extension}\"\n",
        "        elif _mode == \"count\":\n",
        "            question = self.gen_prompt(kind, self.win_mmls, partition, target, state, self.extensions[extension])\n",
        "            baseline = self.generate_baseline(\"win\", _mode, target=\"none\", _start_sector=self.win_partions_dict[partition], case_sensitivity=case_sensitivity, whole_word=whole_word, extension=extension, operator=operator)\n",
        "            elements[\"offset\"] = f\"{self.win_partions_dict[partition]}\"\n",
        "            elements[\"image\"] = \"ss-win-07-25-18.dd\"\n",
        "            elements[\"target\"] = f\"{self.extensions[extension]}\"\n",
        "            elements[\"extension\"] = \"count\"\n",
        "        return {\"question\": question, \"answer\": baseline, \"target\": target, \"eval_elements\": elements}\n",
        "\n",
        "    def unix_gen(self, kind, state, target, partition, case_sensitivity, whole_word, extension, operator, _mode):\n",
        "        elements = {}\n",
        "        if _mode == \"search\":\n",
        "            question = self.gen_prompt(kind, self.unix_mmls, partition, target, state, extension, operator=operator)\n",
        "            baseline = self.generate_baseline(\"unix\", _mode, target=target, _start_sector=self.unix_partions_dict[partition], case_sensitivity=case_sensitivity, whole_word=whole_word, extension=extension, operator=operator)\n",
        "            baseline = self.filter_state(state, baseline)\n",
        "            elements[\"offset\"] = f\"{self.unix_partions_dict[partition]}\"\n",
        "            elements[\"image\"] = \"ss-unix-07-25-18.dd\"\n",
        "            elements[\"extension\"] = f\"{target}\"\n",
        "            elements[\"target\"] = f\"{extension}\"\n",
        "        elif _mode == \"count\":\n",
        "            question = self.gen_prompt(kind, self.unix_mmls, partition, target, state, self.extensions[extension])\n",
        "            baseline = self.generate_baseline(\"unix\", _mode, target=\"none\", _start_sector=self.unix_partions_dict[partition], case_sensitivity=case_sensitivity, whole_word=whole_word, extension=extension, operator=operator)\n",
        "            elements[\"offset\"] = f\"{self.unix_partions_dict[partition]}\"\n",
        "            elements[\"image\"] = \"ss-unix-07-25-18.dd\"\n",
        "            elements[\"extension\"] = f\"{self.extensions[extension]}\"\n",
        "            elements[\"target\"] = \"count\"\n",
        "        return {\"question\": question, \"answer\": baseline, \"target\": target, \"eval_elements\": elements}\n",
        "\n",
        "    def generate_bench(self):\n",
        "        output = []\n",
        "\n",
        "        # Generate for emails\n",
        "        for partition in self.win_partions:\n",
        "            for state in self.file_state:\n",
        "                for email in self.emails:\n",
        "                    temp = self.win_gen(\"email\", state, email, partition, False, False, \"all\", \"or\", \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        for partition in self.unix_partitions:\n",
        "            for state in self.file_state:\n",
        "                for email in self.emails:\n",
        "                    temp = self.unix_gen(\"email\", state, email, partition, False, False, \"all\", \"or\", \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        # Generate for phone numbers\n",
        "        for partition in self.win_partions:\n",
        "            for state in self.file_state:\n",
        "                for phone in self.phones:\n",
        "                    temp = self.win_gen(\"phone\", state, phone, partition, False, False, \"all\", \"or\", \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        for partition in self.unix_partitions:\n",
        "            for phone in self.phones:\n",
        "                for state in self.file_state:\n",
        "                    temp = self.unix_gen(\"phone\", state, phone, partition, False, False, \"all\", \"or\", \"search\")\n",
        "                    output.append(temp)\n",
        "        # Generate for words\n",
        "        for partition in self.win_partions:\n",
        "            for state in self.file_state:\n",
        "                for word in self.words:\n",
        "                    temp = self.win_gen(\"string\", state, word, partition, False, False, \"all\", \"or\", \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        for partition in self.unix_partitions:\n",
        "            for state in self.file_state:\n",
        "                for word in self.words:\n",
        "                    temp = self.unix_gen(\"string\", state, word, partition, False, False, \"all\", \"or\", \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        # Generate word with conditions\n",
        "        # Unfinished\n",
        "        partition = self.win_partions[0]\n",
        "        for word in self.words_condition:\n",
        "            for state in self.file_state:\n",
        "                for operator in self.operators:\n",
        "                    temp = self.win_gen(\"condition\", state, word, partition, False, False, \"all\", operator, \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        partition = self.unix_partitions[0]\n",
        "        for word in self.words_condition:\n",
        "            for state in self.file_state:\n",
        "                for operator in self.operators:\n",
        "                    temp = self.unix_gen(\"condition\", state, word, partition, False, False, \"all\", operator, \"search\")\n",
        "                    output.append(temp)\n",
        "\n",
        "        # count\n",
        "        for partition in self.win_partions:\n",
        "            for extension in self.extensions:\n",
        "                temp = self.win_gen(\"count\", self.file_state[0], extension, partition, False, False, extension, \"or\", \"count\")\n",
        "                output.append(temp)\n",
        "\n",
        "        for partition in self.unix_partitions:\n",
        "            for extension in self.extensions:\n",
        "                temp = self.unix_gen(\"count\", self.file_state[0], extension, partition, False, False, extension, \"or\", \"count\")\n",
        "                output.append(temp)\n",
        "        return output\n",
        "    def export_bench(self, output_file):\n",
        "        raw_data = self.generate_bench()\n",
        "\n",
        "        formatted_data = {\n",
        "            \"dataset\": \"DFIR-Metric Module III (NIST Computer Forensics Tool Testing Program (CFTT) Forensic String Search)\",\n",
        "            \"authors\": \"Bilel Cherif, Aaesha Aldahmani, Saeed Alshehhi, Tamas Bisztray, Richard A. Dubniczky, and Norbert Tihanyi\",\n",
        "            \"sources\": \"https://github.com/DFIR-Metric\",\n",
        "            \"number_of_questions\": len(raw_data),\n",
        "            \"questions\": [\n",
        "                {\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"answer\": item[\"answer\"]\n",
        "                }\n",
        "                for item in raw_data\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(formatted_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "#Generating the NIST JSON\n",
        "output = \"DFIR-Metric-NSS.json\"\n",
        "engine = StringSearchBench()\n",
        "engine.export_bench(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_hIC_dBvWll-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}